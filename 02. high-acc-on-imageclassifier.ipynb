{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achieve high accuracy on image classification wih CIFAR-10\n",
    "in this nootebook we gonne apply some techniques to increase the accuracy \n",
    "\n",
    "we will accomplish the project by following steps:\n",
    "1. Import the dependencies.\n",
    "2. Get the data ready for training:\n",
    "    - Download the data from the Keras library.\n",
    "    - Split it into train, validate, and test datasets.\n",
    "    - Normalize the data.\n",
    "    - One-hot encode the labels.\n",
    "3. Build the model architecture. In addition to regular convolutional and pooling\n",
    "layers, we add the following layers to our architecture:\n",
    "    - Deeper neural network to increase learning capacity\n",
    "    - Dropout layers\n",
    "    - L2 regularization to our convolutional layers\n",
    "    - Batch normalization layers\n",
    "4. Train the model.\n",
    "5. Evaluate the model.\n",
    "6. Plot the learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:33.863391Z",
     "iopub.status.busy": "2024-12-23T09:44:33.863038Z",
     "iopub.status.idle": "2024-12-23T09:44:45.296520Z",
     "shell.execute_reply": "2024-12-23T09:44:45.295687Z",
     "shell.execute_reply.started": "2024-12-23T09:44:33.863359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF:  2.16.1\n",
      "numpy:  1.26.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "print(\"TF: \",tf.__version__)\n",
    "print(\"numpy: \",np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get The Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:45.298537Z",
     "iopub.status.busy": "2024-12-23T09:44:45.298118Z",
     "iopub.status.idle": "2024-12-23T09:44:50.201433Z",
     "shell.execute_reply": "2024-12-23T09:44:50.200464Z",
     "shell.execute_reply.started": "2024-12-23T09:44:45.298510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "x_train : (45000, 32, 32, 3)\n",
      "x_valid : (5000, 32, 32, 3)\n",
      "x_test : (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test)= cifar10.load_data()\n",
    "x_train= x_train.astype('float32')\n",
    "x_test= x_test.astype('float32')\n",
    "\n",
    "(x_train , x_valid)= x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid)= y_train[5000:], y_train[:5000]\n",
    "\n",
    "print(f'x_train : {x_train.shape}')\n",
    "print(f'x_valid : {x_valid.shape}')\n",
    "print(f'x_test : {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the Data**\n",
    "\n",
    "Normalizing the pixel values of our images is done by subtracting the mean from each\n",
    "pixel and then dividing the result by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:50.203809Z",
     "iopub.status.busy": "2024-12-23T09:44:50.202861Z",
     "iopub.status.idle": "2024-12-23T09:44:50.859577Z",
     "shell.execute_reply": "2024-12-23T09:44:50.858883Z",
     "shell.execute_reply.started": "2024-12-23T09:44:50.203764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mean= np.mean(x_train,axis=(0,1,2,3))\n",
    "std= np.std(x_train,axis=(0,1,2,3))\n",
    "\n",
    "x_train=(x_train - mean) / (std+1e-7)\n",
    "x_valid=(x_valid - mean) / (std+1e-7)\n",
    "x_test=(x_test - mean) / (std+1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:50.862531Z",
     "iopub.status.busy": "2024-12-23T09:44:50.861725Z",
     "iopub.status.idle": "2024-12-23T09:44:50.871334Z",
     "shell.execute_reply": "2024-12-23T09:44:50.870393Z",
     "shell.execute_reply.started": "2024-12-23T09:44:50.862487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_classes= 10\n",
    "y_train= to_categorical(y_train,num_classes)\n",
    "y_valid= to_categorical(y_valid,num_classes)\n",
    "y_test= to_categorical(y_test,num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:50.872830Z",
     "iopub.status.busy": "2024-12-23T09:44:50.872562Z",
     "iopub.status.idle": "2024-12-23T09:44:51.320815Z",
     "shell.execute_reply": "2024-12-23T09:44:51.320132Z",
     "shell.execute_reply.started": "2024-12-23T09:44:50.872805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datagn=ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    \n",
    ")\n",
    "datagn.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model Architure\n",
    "\n",
    "This Architucture inspired by VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:44:51.322033Z",
     "iopub.status.busy": "2024-12-23T09:44:51.321783Z",
     "iopub.status.idle": "2024-12-23T09:44:52.425322Z",
     "shell.execute_reply": "2024-12-23T09:44:52.424522Z",
     "shell.execute_reply.started": "2024-12-23T09:44:51.322009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m20,490\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">309,290</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m309,290\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,394</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m308,394\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_hidden_units=32\n",
    "weight_decay=1e-4\n",
    "\n",
    "model= tf.keras.Sequential()\n",
    "\n",
    "# CONV-1\n",
    "model.add(Conv2D(base_hidden_units,kernel_size=3,padding='same',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                          input_shape=x_train.shape[1:] ,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# CONV-2\n",
    "model.add(Conv2D(base_hidden_units,kernel_size=3,padding='same',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                            activation='relu'))\n",
    "#model.add(Activation='relu')\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# pool + droupout\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# CONV-3\n",
    "model.add(Conv2D(base_hidden_units *2 , kernel_size=3,padding='same',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                 activation='relu'\n",
    "                ))\n",
    "#model.add(Activation='relu')\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Conv-4\n",
    "model.add(Conv2D(base_hidden_units *2 , kernel_size=3,padding='same',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                 activation='relu'\n",
    "                ))\n",
    "#model.add(Activation='relu')\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# POOL + Droupuot\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Conv-5\n",
    "model.add(Conv2D(base_hidden_units*4, kernel_size=3,padding='same',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                activation='relu'))\n",
    "#model.add(Activation='relu')\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Conv-6\n",
    "model.add(Conv2D(base_hidden_units*4, kernel_size=3,padding='same',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                activation='relu'))\n",
    "#model.add(Activation='relu')\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# POOl + Dropout\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# FC7\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:46:26.566294Z",
     "iopub.status.busy": "2024-12-23T09:46:26.565946Z",
     "iopub.status.idle": "2024-12-23T10:37:00.090380Z",
     "shell.execute_reply": "2024-12-23T10:37:00.089503Z",
     "shell.execute_reply.started": "2024-12-23T09:46:26.566264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734947190.704745      95 service.cc:145] XLA service 0x7cabb800b2d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734947190.704801      95 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1734947190.704808      95 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1734947198.922373      95 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.02021, saving model to model.125epochs.keras\n",
      "45000/45000 - 44s - 973us/step - accuracy: 0.2670 - loss: 2.9056 - val_accuracy: 0.3106 - val_loss: 2.0202\n",
      "Epoch 2/125\n",
      "\n",
      "Epoch 2: val_loss improved from 2.02021 to 1.55866, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.3473 - loss: 2.1839 - val_accuracy: 0.4792 - val_loss: 1.5587\n",
      "Epoch 3/125\n",
      "\n",
      "Epoch 3: val_loss improved from 1.55866 to 1.46715, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.3975 - loss: 1.9084 - val_accuracy: 0.5032 - val_loss: 1.4672\n",
      "Epoch 4/125\n",
      "\n",
      "Epoch 4: val_loss improved from 1.46715 to 1.41597, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 528us/step - accuracy: 0.4334 - loss: 1.7527 - val_accuracy: 0.5172 - val_loss: 1.4160\n",
      "Epoch 5/125\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.41597\n",
      "45000/45000 - 24s - 532us/step - accuracy: 0.4616 - loss: 1.6425 - val_accuracy: 0.5126 - val_loss: 1.4463\n",
      "Epoch 6/125\n",
      "\n",
      "Epoch 6: val_loss improved from 1.41597 to 1.41574, saving model to model.125epochs.keras\n",
      "45000/45000 - 25s - 560us/step - accuracy: 0.4862 - loss: 1.5573 - val_accuracy: 0.5232 - val_loss: 1.4157\n",
      "Epoch 7/125\n",
      "\n",
      "Epoch 7: val_loss improved from 1.41574 to 1.30179, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 542us/step - accuracy: 0.5115 - loss: 1.4836 - val_accuracy: 0.5552 - val_loss: 1.3018\n",
      "Epoch 8/125\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.30179\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.5287 - loss: 1.4222 - val_accuracy: 0.5528 - val_loss: 1.3208\n",
      "Epoch 9/125\n",
      "\n",
      "Epoch 9: val_loss improved from 1.30179 to 1.22021, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 538us/step - accuracy: 0.5452 - loss: 1.3675 - val_accuracy: 0.5852 - val_loss: 1.2202\n",
      "Epoch 10/125\n",
      "\n",
      "Epoch 10: val_loss improved from 1.22021 to 1.21670, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.5639 - loss: 1.3092 - val_accuracy: 0.5854 - val_loss: 1.2167\n",
      "Epoch 11/125\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.21670\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.5812 - loss: 1.2619 - val_accuracy: 0.5890 - val_loss: 1.2469\n",
      "Epoch 12/125\n",
      "\n",
      "Epoch 12: val_loss improved from 1.21670 to 1.14071, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.5901 - loss: 1.2269 - val_accuracy: 0.6150 - val_loss: 1.1407\n",
      "Epoch 13/125\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.14071\n",
      "45000/45000 - 24s - 542us/step - accuracy: 0.6072 - loss: 1.1753 - val_accuracy: 0.6116 - val_loss: 1.1694\n",
      "Epoch 14/125\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.14071\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.6193 - loss: 1.1440 - val_accuracy: 0.6174 - val_loss: 1.1589\n",
      "Epoch 15/125\n",
      "\n",
      "Epoch 15: val_loss improved from 1.14071 to 1.06798, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.6266 - loss: 1.1104 - val_accuracy: 0.6458 - val_loss: 1.0680\n",
      "Epoch 16/125\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.06798\n",
      "45000/45000 - 25s - 546us/step - accuracy: 0.6412 - loss: 1.0724 - val_accuracy: 0.6532 - val_loss: 1.0729\n",
      "Epoch 17/125\n",
      "\n",
      "Epoch 17: val_loss improved from 1.06798 to 0.98337, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 532us/step - accuracy: 0.6494 - loss: 1.0480 - val_accuracy: 0.6770 - val_loss: 0.9834\n",
      "Epoch 18/125\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.98337\n",
      "45000/45000 - 25s - 547us/step - accuracy: 0.6550 - loss: 1.0308 - val_accuracy: 0.6592 - val_loss: 1.0611\n",
      "Epoch 19/125\n",
      "\n",
      "Epoch 19: val_loss improved from 0.98337 to 0.95339, saving model to model.125epochs.keras\n",
      "45000/45000 - 25s - 552us/step - accuracy: 0.6655 - loss: 0.9977 - val_accuracy: 0.6850 - val_loss: 0.9534\n",
      "Epoch 20/125\n",
      "\n",
      "Epoch 20: val_loss improved from 0.95339 to 0.90226, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.6703 - loss: 0.9791 - val_accuracy: 0.7022 - val_loss: 0.9023\n",
      "Epoch 21/125\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.90226\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.6826 - loss: 0.9480 - val_accuracy: 0.6904 - val_loss: 0.9365\n",
      "Epoch 22/125\n",
      "\n",
      "Epoch 22: val_loss improved from 0.90226 to 0.87387, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.6862 - loss: 0.9376 - val_accuracy: 0.7136 - val_loss: 0.8739\n",
      "Epoch 23/125\n",
      "\n",
      "Epoch 23: val_loss improved from 0.87387 to 0.87260, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.6952 - loss: 0.9122 - val_accuracy: 0.7140 - val_loss: 0.8726\n",
      "Epoch 24/125\n",
      "\n",
      "Epoch 24: val_loss improved from 0.87260 to 0.84695, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.6993 - loss: 0.8975 - val_accuracy: 0.7204 - val_loss: 0.8469\n",
      "Epoch 25/125\n",
      "\n",
      "Epoch 25: val_loss improved from 0.84695 to 0.83646, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7068 - loss: 0.8790 - val_accuracy: 0.7260 - val_loss: 0.8365\n",
      "Epoch 26/125\n",
      "\n",
      "Epoch 26: val_loss improved from 0.83646 to 0.79980, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7088 - loss: 0.8623 - val_accuracy: 0.7306 - val_loss: 0.7998\n",
      "Epoch 27/125\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.79980\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7164 - loss: 0.8475 - val_accuracy: 0.7334 - val_loss: 0.8268\n",
      "Epoch 28/125\n",
      "\n",
      "Epoch 28: val_loss improved from 0.79980 to 0.79862, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7211 - loss: 0.8329 - val_accuracy: 0.7446 - val_loss: 0.7986\n",
      "Epoch 29/125\n",
      "\n",
      "Epoch 29: val_loss improved from 0.79862 to 0.79142, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7251 - loss: 0.8151 - val_accuracy: 0.7438 - val_loss: 0.7914\n",
      "Epoch 30/125\n",
      "\n",
      "Epoch 30: val_loss improved from 0.79142 to 0.75287, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.7295 - loss: 0.8049 - val_accuracy: 0.7512 - val_loss: 0.7529\n",
      "Epoch 31/125\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.75287\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.7337 - loss: 0.7962 - val_accuracy: 0.7468 - val_loss: 0.7856\n",
      "Epoch 32/125\n",
      "\n",
      "Epoch 32: val_loss improved from 0.75287 to 0.72080, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7374 - loss: 0.7838 - val_accuracy: 0.7666 - val_loss: 0.7208\n",
      "Epoch 33/125\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.72080\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7409 - loss: 0.7743 - val_accuracy: 0.7636 - val_loss: 0.7218\n",
      "Epoch 34/125\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.72080\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.7450 - loss: 0.7688 - val_accuracy: 0.7490 - val_loss: 0.7636\n",
      "Epoch 35/125\n",
      "\n",
      "Epoch 35: val_loss improved from 0.72080 to 0.70538, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7477 - loss: 0.7562 - val_accuracy: 0.7678 - val_loss: 0.7054\n",
      "Epoch 36/125\n",
      "\n",
      "Epoch 36: val_loss improved from 0.70538 to 0.68499, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7500 - loss: 0.7501 - val_accuracy: 0.7758 - val_loss: 0.6850\n",
      "Epoch 37/125\n",
      "\n",
      "Epoch 37: val_loss improved from 0.68499 to 0.68434, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.7540 - loss: 0.7366 - val_accuracy: 0.7764 - val_loss: 0.6843\n",
      "Epoch 38/125\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.68434\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.7597 - loss: 0.7223 - val_accuracy: 0.7750 - val_loss: 0.6857\n",
      "Epoch 39/125\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.68434\n",
      "45000/45000 - 24s - 535us/step - accuracy: 0.7604 - loss: 0.7125 - val_accuracy: 0.7766 - val_loss: 0.6879\n",
      "Epoch 40/125\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.68434\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.7634 - loss: 0.7137 - val_accuracy: 0.7760 - val_loss: 0.6953\n",
      "Epoch 41/125\n",
      "\n",
      "Epoch 41: val_loss improved from 0.68434 to 0.66175, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.7670 - loss: 0.7000 - val_accuracy: 0.7848 - val_loss: 0.6617\n",
      "Epoch 42/125\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.66175\n",
      "45000/45000 - 24s - 538us/step - accuracy: 0.7689 - loss: 0.6946 - val_accuracy: 0.7744 - val_loss: 0.7087\n",
      "Epoch 43/125\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.66175\n",
      "45000/45000 - 25s - 548us/step - accuracy: 0.7704 - loss: 0.6870 - val_accuracy: 0.7798 - val_loss: 0.6932\n",
      "Epoch 44/125\n",
      "\n",
      "Epoch 44: val_loss improved from 0.66175 to 0.63139, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.7742 - loss: 0.6781 - val_accuracy: 0.7980 - val_loss: 0.6314\n",
      "Epoch 45/125\n",
      "\n",
      "Epoch 45: val_loss improved from 0.63139 to 0.61309, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 542us/step - accuracy: 0.7760 - loss: 0.6744 - val_accuracy: 0.8034 - val_loss: 0.6131\n",
      "Epoch 46/125\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.61309\n",
      "45000/45000 - 24s - 544us/step - accuracy: 0.7784 - loss: 0.6682 - val_accuracy: 0.7918 - val_loss: 0.6487\n",
      "Epoch 47/125\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.61309\n",
      "45000/45000 - 24s - 538us/step - accuracy: 0.7786 - loss: 0.6642 - val_accuracy: 0.7952 - val_loss: 0.6400\n",
      "Epoch 48/125\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.61309\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.7813 - loss: 0.6566 - val_accuracy: 0.8054 - val_loss: 0.6143\n",
      "Epoch 49/125\n",
      "\n",
      "Epoch 49: val_loss improved from 0.61309 to 0.60221, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7839 - loss: 0.6517 - val_accuracy: 0.8122 - val_loss: 0.6022\n",
      "Epoch 50/125\n",
      "\n",
      "Epoch 50: val_loss improved from 0.60221 to 0.57992, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.7862 - loss: 0.6438 - val_accuracy: 0.8136 - val_loss: 0.5799\n",
      "Epoch 51/125\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.57992\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.7887 - loss: 0.6398 - val_accuracy: 0.8134 - val_loss: 0.5901\n",
      "Epoch 52/125\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.57992\n",
      "45000/45000 - 24s - 535us/step - accuracy: 0.7903 - loss: 0.6323 - val_accuracy: 0.8114 - val_loss: 0.5913\n",
      "Epoch 53/125\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.57992\n",
      "45000/45000 - 24s - 537us/step - accuracy: 0.7932 - loss: 0.6275 - val_accuracy: 0.8134 - val_loss: 0.6039\n",
      "Epoch 54/125\n",
      "\n",
      "Epoch 54: val_loss improved from 0.57992 to 0.56449, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.7946 - loss: 0.6207 - val_accuracy: 0.8226 - val_loss: 0.5645\n",
      "Epoch 55/125\n",
      "\n",
      "Epoch 55: val_loss improved from 0.56449 to 0.56018, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.7985 - loss: 0.6156 - val_accuracy: 0.8304 - val_loss: 0.5602\n",
      "Epoch 56/125\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.56018\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.8014 - loss: 0.6060 - val_accuracy: 0.8106 - val_loss: 0.6093\n",
      "Epoch 57/125\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.56018\n",
      "45000/45000 - 25s - 549us/step - accuracy: 0.7981 - loss: 0.6103 - val_accuracy: 0.8220 - val_loss: 0.5833\n",
      "Epoch 58/125\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.56018\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.8029 - loss: 0.6023 - val_accuracy: 0.8208 - val_loss: 0.5809\n",
      "Epoch 59/125\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.56018\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.8013 - loss: 0.6016 - val_accuracy: 0.8196 - val_loss: 0.5690\n",
      "Epoch 60/125\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.56018\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8038 - loss: 0.5950 - val_accuracy: 0.8190 - val_loss: 0.5925\n",
      "Epoch 61/125\n",
      "\n",
      "Epoch 61: val_loss improved from 0.56018 to 0.53985, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 536us/step - accuracy: 0.8080 - loss: 0.5850 - val_accuracy: 0.8338 - val_loss: 0.5399\n",
      "Epoch 62/125\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.53985\n",
      "45000/45000 - 24s - 541us/step - accuracy: 0.8077 - loss: 0.5863 - val_accuracy: 0.8282 - val_loss: 0.5493\n",
      "Epoch 63/125\n",
      "\n",
      "Epoch 63: val_loss improved from 0.53985 to 0.53460, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 543us/step - accuracy: 0.8126 - loss: 0.5777 - val_accuracy: 0.8328 - val_loss: 0.5346\n",
      "Epoch 64/125\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.53460\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8111 - loss: 0.5727 - val_accuracy: 0.8300 - val_loss: 0.5397\n",
      "Epoch 65/125\n",
      "\n",
      "Epoch 65: val_loss improved from 0.53460 to 0.51729, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 531us/step - accuracy: 0.8110 - loss: 0.5745 - val_accuracy: 0.8408 - val_loss: 0.5173\n",
      "Epoch 66/125\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.51729\n",
      "45000/45000 - 24s - 535us/step - accuracy: 0.8138 - loss: 0.5639 - val_accuracy: 0.8292 - val_loss: 0.5563\n",
      "Epoch 67/125\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.51729\n",
      "45000/45000 - 24s - 541us/step - accuracy: 0.8132 - loss: 0.5623 - val_accuracy: 0.8336 - val_loss: 0.5446\n",
      "Epoch 68/125\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.51729\n",
      "45000/45000 - 24s - 530us/step - accuracy: 0.8136 - loss: 0.5636 - val_accuracy: 0.8348 - val_loss: 0.5247\n",
      "Epoch 69/125\n",
      "\n",
      "Epoch 69: val_loss improved from 0.51729 to 0.51318, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 535us/step - accuracy: 0.8168 - loss: 0.5539 - val_accuracy: 0.8406 - val_loss: 0.5132\n",
      "Epoch 70/125\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.51318\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.8204 - loss: 0.5512 - val_accuracy: 0.8380 - val_loss: 0.5188\n",
      "Epoch 71/125\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.51318\n",
      "45000/45000 - 24s - 529us/step - accuracy: 0.8177 - loss: 0.5521 - val_accuracy: 0.8336 - val_loss: 0.5293\n",
      "Epoch 72/125\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.51318\n",
      "45000/45000 - 24s - 530us/step - accuracy: 0.8232 - loss: 0.5347 - val_accuracy: 0.8380 - val_loss: 0.5309\n",
      "Epoch 73/125\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.51318\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.8225 - loss: 0.5477 - val_accuracy: 0.8404 - val_loss: 0.5175\n",
      "Epoch 74/125\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.51318\n",
      "45000/45000 - 25s - 544us/step - accuracy: 0.8236 - loss: 0.5391 - val_accuracy: 0.8304 - val_loss: 0.5434\n",
      "Epoch 75/125\n",
      "\n",
      "Epoch 75: val_loss improved from 0.51318 to 0.48503, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 532us/step - accuracy: 0.8246 - loss: 0.5378 - val_accuracy: 0.8478 - val_loss: 0.4850\n",
      "Epoch 76/125\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.48503\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.8261 - loss: 0.5353 - val_accuracy: 0.8398 - val_loss: 0.5073\n",
      "Epoch 77/125\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.48503\n",
      "45000/45000 - 24s - 532us/step - accuracy: 0.8269 - loss: 0.5278 - val_accuracy: 0.8430 - val_loss: 0.4937\n",
      "Epoch 78/125\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.48503\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.8269 - loss: 0.5253 - val_accuracy: 0.8416 - val_loss: 0.5180\n",
      "Epoch 79/125\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.48503\n",
      "45000/45000 - 24s - 542us/step - accuracy: 0.8264 - loss: 0.5246 - val_accuracy: 0.8436 - val_loss: 0.5148\n",
      "Epoch 80/125\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.48503\n",
      "45000/45000 - 24s - 541us/step - accuracy: 0.8303 - loss: 0.5238 - val_accuracy: 0.8476 - val_loss: 0.4937\n",
      "Epoch 81/125\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.48503\n",
      "45000/45000 - 23s - 521us/step - accuracy: 0.8324 - loss: 0.5164 - val_accuracy: 0.8472 - val_loss: 0.5045\n",
      "Epoch 82/125\n",
      "\n",
      "Epoch 82: val_loss improved from 0.48503 to 0.47404, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 531us/step - accuracy: 0.8300 - loss: 0.5186 - val_accuracy: 0.8540 - val_loss: 0.4740\n",
      "Epoch 83/125\n",
      "\n",
      "Epoch 83: val_loss improved from 0.47404 to 0.47363, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.8330 - loss: 0.5117 - val_accuracy: 0.8528 - val_loss: 0.4736\n",
      "Epoch 84/125\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.47363\n",
      "45000/45000 - 24s - 529us/step - accuracy: 0.8355 - loss: 0.5059 - val_accuracy: 0.8490 - val_loss: 0.4794\n",
      "Epoch 85/125\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.47363\n",
      "45000/45000 - 24s - 529us/step - accuracy: 0.8358 - loss: 0.5068 - val_accuracy: 0.8530 - val_loss: 0.4773\n",
      "Epoch 86/125\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.47363\n",
      "45000/45000 - 24s - 532us/step - accuracy: 0.8357 - loss: 0.5041 - val_accuracy: 0.8486 - val_loss: 0.4951\n",
      "Epoch 87/125\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.47363\n",
      "45000/45000 - 25s - 557us/step - accuracy: 0.8364 - loss: 0.5002 - val_accuracy: 0.8524 - val_loss: 0.4869\n",
      "Epoch 88/125\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.47363\n",
      "45000/45000 - 25s - 563us/step - accuracy: 0.8389 - loss: 0.4957 - val_accuracy: 0.8540 - val_loss: 0.4744\n",
      "Epoch 89/125\n",
      "\n",
      "Epoch 89: val_loss improved from 0.47363 to 0.47287, saving model to model.125epochs.keras\n",
      "45000/45000 - 25s - 563us/step - accuracy: 0.8387 - loss: 0.4981 - val_accuracy: 0.8582 - val_loss: 0.4729\n",
      "Epoch 90/125\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.47287\n",
      "45000/45000 - 25s - 549us/step - accuracy: 0.8390 - loss: 0.4943 - val_accuracy: 0.8482 - val_loss: 0.4967\n",
      "Epoch 91/125\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.47287\n",
      "45000/45000 - 25s - 567us/step - accuracy: 0.8393 - loss: 0.4953 - val_accuracy: 0.8468 - val_loss: 0.4935\n",
      "Epoch 92/125\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.47287\n",
      "45000/45000 - 25s - 565us/step - accuracy: 0.8423 - loss: 0.4901 - val_accuracy: 0.8518 - val_loss: 0.4765\n",
      "Epoch 93/125\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.47287\n",
      "45000/45000 - 24s - 541us/step - accuracy: 0.8423 - loss: 0.4861 - val_accuracy: 0.8496 - val_loss: 0.4899\n",
      "Epoch 94/125\n",
      "\n",
      "Epoch 94: val_loss improved from 0.47287 to 0.45547, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.8433 - loss: 0.4841 - val_accuracy: 0.8614 - val_loss: 0.4555\n",
      "Epoch 95/125\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.8422 - loss: 0.4849 - val_accuracy: 0.8484 - val_loss: 0.4993\n",
      "Epoch 96/125\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.8444 - loss: 0.4786 - val_accuracy: 0.8460 - val_loss: 0.5033\n",
      "Epoch 97/125\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 540us/step - accuracy: 0.8449 - loss: 0.4825 - val_accuracy: 0.8516 - val_loss: 0.4854\n",
      "Epoch 98/125\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.8439 - loss: 0.4779 - val_accuracy: 0.8538 - val_loss: 0.4799\n",
      "Epoch 99/125\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 531us/step - accuracy: 0.8445 - loss: 0.4760 - val_accuracy: 0.8518 - val_loss: 0.4952\n",
      "Epoch 100/125\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.8448 - loss: 0.4769 - val_accuracy: 0.8534 - val_loss: 0.4861\n",
      "Epoch 101/125\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.8475 - loss: 0.4728 - val_accuracy: 0.8576 - val_loss: 0.4654\n",
      "Epoch 102/125\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 535us/step - accuracy: 0.8487 - loss: 0.4660 - val_accuracy: 0.8552 - val_loss: 0.4781\n",
      "Epoch 103/125\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 541us/step - accuracy: 0.8480 - loss: 0.4681 - val_accuracy: 0.8584 - val_loss: 0.4735\n",
      "Epoch 104/125\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.8492 - loss: 0.4646 - val_accuracy: 0.8578 - val_loss: 0.4675\n",
      "Epoch 105/125\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.45547\n",
      "45000/45000 - 23s - 521us/step - accuracy: 0.8502 - loss: 0.4636 - val_accuracy: 0.8608 - val_loss: 0.4582\n",
      "Epoch 106/125\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8502 - loss: 0.4625 - val_accuracy: 0.8574 - val_loss: 0.4726\n",
      "Epoch 107/125\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8524 - loss: 0.4554 - val_accuracy: 0.8534 - val_loss: 0.4851\n",
      "Epoch 108/125\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 523us/step - accuracy: 0.8520 - loss: 0.4548 - val_accuracy: 0.8634 - val_loss: 0.4590\n",
      "Epoch 109/125\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 523us/step - accuracy: 0.8529 - loss: 0.4578 - val_accuracy: 0.8648 - val_loss: 0.4575\n",
      "Epoch 110/125\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 530us/step - accuracy: 0.8535 - loss: 0.4536 - val_accuracy: 0.8652 - val_loss: 0.4604\n",
      "Epoch 111/125\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 525us/step - accuracy: 0.8544 - loss: 0.4532 - val_accuracy: 0.8526 - val_loss: 0.4986\n",
      "Epoch 112/125\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 528us/step - accuracy: 0.8526 - loss: 0.4503 - val_accuracy: 0.8620 - val_loss: 0.4572\n",
      "Epoch 113/125\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8557 - loss: 0.4515 - val_accuracy: 0.8620 - val_loss: 0.4559\n",
      "Epoch 114/125\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 525us/step - accuracy: 0.8564 - loss: 0.4455 - val_accuracy: 0.8582 - val_loss: 0.4643\n",
      "Epoch 115/125\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.45547\n",
      "45000/45000 - 24s - 524us/step - accuracy: 0.8561 - loss: 0.4450 - val_accuracy: 0.8636 - val_loss: 0.4563\n",
      "Epoch 116/125\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.45547\n",
      "45000/45000 - 23s - 518us/step - accuracy: 0.8586 - loss: 0.4404 - val_accuracy: 0.8604 - val_loss: 0.4646\n",
      "Epoch 117/125\n",
      "\n",
      "Epoch 117: val_loss improved from 0.45547 to 0.44975, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 531us/step - accuracy: 0.8553 - loss: 0.4493 - val_accuracy: 0.8654 - val_loss: 0.4498\n",
      "Epoch 118/125\n",
      "\n",
      "Epoch 118: val_loss improved from 0.44975 to 0.44911, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 527us/step - accuracy: 0.8590 - loss: 0.4358 - val_accuracy: 0.8698 - val_loss: 0.4491\n",
      "Epoch 119/125\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.44911\n",
      "45000/45000 - 24s - 526us/step - accuracy: 0.8606 - loss: 0.4353 - val_accuracy: 0.8632 - val_loss: 0.4644\n",
      "Epoch 120/125\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.44911\n",
      "45000/45000 - 24s - 523us/step - accuracy: 0.8598 - loss: 0.4384 - val_accuracy: 0.8646 - val_loss: 0.4505\n",
      "Epoch 121/125\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.44911\n",
      "45000/45000 - 23s - 521us/step - accuracy: 0.8600 - loss: 0.4328 - val_accuracy: 0.8576 - val_loss: 0.4670\n",
      "Epoch 122/125\n",
      "\n",
      "Epoch 122: val_loss improved from 0.44911 to 0.43759, saving model to model.125epochs.keras\n",
      "45000/45000 - 24s - 539us/step - accuracy: 0.8635 - loss: 0.4303 - val_accuracy: 0.8700 - val_loss: 0.4376\n",
      "Epoch 123/125\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.43759\n",
      "45000/45000 - 24s - 534us/step - accuracy: 0.8594 - loss: 0.4342 - val_accuracy: 0.8676 - val_loss: 0.4428\n",
      "Epoch 124/125\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.43759\n",
      "45000/45000 - 24s - 529us/step - accuracy: 0.8612 - loss: 0.4353 - val_accuracy: 0.8662 - val_loss: 0.4495\n",
      "Epoch 125/125\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.43759\n",
      "45000/45000 - 24s - 533us/step - accuracy: 0.8623 - loss: 0.4307 - val_accuracy: 0.8648 - val_loss: 0.4512\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=125\n",
    "\n",
    "checkpointer= ModelCheckpoint(filepath='model.125epochs.keras',verbose=1,save_best_only=True)\n",
    "optimizer= tf.keras.optimizers.Adam(learning_rate=0.0001,decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(datagn.flow(x_train,y_train, batch_size=batch_size),\n",
    "                   callbacks=[checkpointer],\n",
    "                   epochs=epochs,\n",
    "                  steps_per_epoch=x_train.shape[0],verbose=2,validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:40:42.309324Z",
     "iopub.status.busy": "2024-12-23T10:40:42.308974Z",
     "iopub.status.idle": "2024-12-23T10:40:43.090365Z",
     "shell.execute_reply": "2024-12-23T10:40:43.089536Z",
     "shell.execute_reply.started": "2024-12-23T10:40:42.309294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8502 - loss: 0.4925\n",
      "Test result :85.14000177383423 \n",
      "Loss: 0.4936971068382263\n"
     ]
    }
   ],
   "source": [
    "scores= model.evaluate(x_test,y_test,batch_size=128, verbose=1)\n",
    "\n",
    "print(f'Test result :{scores[1]*100 } \\nLoss: {scores[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:37:42.411604Z",
     "iopub.status.busy": "2024-12-23T10:37:42.411270Z",
     "iopub.status.idle": "2024-12-23T10:37:42.639145Z",
     "shell.execute_reply": "2024-12-23T10:37:42.638275Z",
     "shell.execute_reply.started": "2024-12-23T10:37:42.411574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
